# BERT 增量微调二分类模型

## 项目概述

本项目利用BERT预训练模型进行增量微调，实现中文文本的二分类任务。项目基于情感分析数据集ChnSentiCorp，通过冻结BERT参数并添加一个简单的全连接层，高效地实现了正负向情感分类功能。

## 模型架构

模型采用了BERT预训练模型 + 增量微调的方式:
- 使用预训练的中文BERT模型作为特征提取器（参数冻结）
- 添加一个全连接层（768→2）作为分类头进行二分类
- 仅训练分类头部分，大幅减少了训练参数和计算资源需求

## 项目结构

```
bert/
│
├── train.py        # 模型训练脚本
├── run.py          # 模型推理脚本
├── net.py          # 模型定义
├── MyData.py       # 数据集加载
├── token_test.py   # 分词测试
├── data_test.py    # 数据集测试
│
├── data/           # 存放数据集
│
├── model/          # BERT预训练模型存放位置
│
└── params/         # 训练好的模型参数保存位置
```

## 使用指南

### 环境要求

- Python 3.6+
- PyTorch 1.7+
- Transformers 4.0+
- Datasets

### 安装依赖

```bash
pip install -r  requirements.txt
```

### 数据准备

本项目使用的是ChnSentiCorp中文情感分析数据集，请确保数据集已正确加载至`data/`目录。

### 训练模型

运行以下命令开始训练模型：

```bash
python train.py
```

默认训练轮次为30000，可在`train.py`中修改`EPOCH`变量调整训练轮次。训练过程中模型参数会自动保存在`params/`目录下。

### 模型推理

训练完成后，运行以下命令进行模型推理测试：

```bash
python run.py
```

运行后可以输入任意中文文本，模型将输出分类结果（正向评价或负向评价）。

## 模型特点

1. **参数高效**：通过冻结BERT基础模型参数，只训练分类头，大幅降低训练开销
2. **迁移学习**：利用预训练模型的知识迁移能力，在小数据集上也能取得良好效果
3. **灵活易用**：模型结构简单，易于理解和修改

## 性能评估

模型在训练过程中会输出训练损失和准确率。训练完成后的模型在情感分析任务上能够有效区分正负向评价。

## 扩展方向

1. 尝试不同的BERT变种模型（如RoBERTa, DistilBERT等）
2. 改变分类头结构，如添加多层MLP
3. 尝试解冻BERT部分层进行微调，平衡效果和训练成本
4. 扩展到多分类任务 